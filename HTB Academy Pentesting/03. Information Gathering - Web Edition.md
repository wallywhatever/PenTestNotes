## Information Areas

- **Domains and Subdomains**
	- We may be given a single domain or list of subdomains. Many orgs don't have good asset inventory and may have forgotten domains and subdomains exposed externally. We may come accross various subdomains that map back to in-scope IPs, increasing the overall attack surface. Hidden and forgotten subdomains may have old/vulnerable versions of apps or dev versions with debugging functionality. It's also possible to find subdomains of subdomains.

- **IP Ranges**
	- Unless constrained to a very specific scope, we want to find out as much as possible about the target. Finding additional IP ranges owned by our target may lead to discovering domains and subdomains.
	
- **Infrastructure**
	- We need to know what technology stacks our target is using. What types of APIs/web services are in use? Are they using CMS like WorldPress, Joomla, Drupal, or DotNetNuke which have their own vulns and misconfigurations. We also care about web servers in use like, IIS, Nginx, or Apache as well as the version numbers. What type of back-end databases are in use (MSSQL, MySQL, PostgreSQL, SQLLite, Oracle, etc)?

- **Virtual Hosts**
	- We want to enumerate virtual hosts (vhosts) which are similiar to subdomains but indicate that an org is hosting multiple applications on the same webserver.

___
# Passive Information Gathering

## WHOIS
- The "white pages" for domain names.
- TCP query response on port 43.
- Allows us to retrieve info about the domain name of an already registered domain.
	- `whois "<FQDN/IP>"`

## DNS
#### Nslookup/Dig
- Searches DNS servers on the Internet and asks them for information about hosts and domains.
- A records can be queried just by submitting a domain name but the `-query` paramater allows us to search specific records.
	- `nslookup <FQDN>`
		- To specify a nameserver add `@<nameserver/IP>`
		- `-query=A`
		- `-query=PTR`
		- `-query=ANY` - deprecated, may still work
		- `-query=TXT`
		- `-query=MX`
- `dig <record> <FQDN> [@<DNSServer>]`

## Passive Subdomain Enumeration

#### [VirusTotal](https://www.virustotal.com/gui/home/url)
- VT maintains its DNS replication service, which is developed by preserving DNS resolutions made when users vist URLs given by them.
- Search a URL and then click the `Relations` tab

### Certificates
- [https://censys.io](https://censys.io/)
- [crt.sh](https://crt.sh/) - down 3/21/23

- To get a JSON formatted list from crt.sh
- `curl -s "https://crt.sh/?q=${TARGET}&output=json" | jq -r '.[] | "\(.name_value)\n\(.common_name)"' | sort -u > "${TARGET}_crt.sh.txt"`
![[2023-03-21 13_39_10-Information Gathering - Web Edition.png|700]]

We can also manually perform this against a target using OpenSSL
``` bash
export TARGET="facebook.com"
export PORT="443"
openssl s_client -ign_eof 2>/dev/null <<<$'HEAD / HTTP/1.0\r\n\r' -connect "${TARGET}:${PORT}" | openssl x509 -noout -text -in - | grep 'DNS' | sed -e 's|DNS:|\n|g' -e 's|^\*.*||g' | tr -d ',' | sort -u
```

#### Automating Passive Subdomain Enumeration
- [TheHarvester](https://github.com/laramies/theHarvester) is a simple to use tool to identify a company's attack surface.
	
- For now we will use the following modules:
	- [Baidu](http://www.baidu.com/) - Baidu search engine.
	- `Bufferoverun` - Uses data from Rapid7's[ Project Sonar](http://www.rapid7.com/research/project-sonar/)
	- [Crtsh](https://crt.sh/) - Comodo Certificate search.
	- [Hackertarget](https://hackertarget.com/) - Online vulnerability scanners and network intelligence to help organizations.
	- [Otx](https://otx.alienvault.com/))- AlienVault Open Threat Exchange 
	- [Rapiddns](https://rapiddns.io/) - DNS query tool, which makes querying subdomains or sites using the same IP easy.
	- [Sublist3r](https://github.com/aboul3la/Sublist3r) - Fast subdomains enumeration tool for penetration testers
	- [Threatcrowd](http://www.threatcrowd.org/) - Open source threat intelligence.
	- [Threatminer](https://www.threatminer.org/) - Data mining for threat intelligence.
	- `Trello` - Search Trello boards (Uses Google search)
	- [Urlscan](https://urlscan.io/)- A sandbox for the web that is a URL and website scanner.
	- `Vhost` - Bing virtual hosts search.
	- [Virustotal](https://www.virustotal.com/gui/home/search) - Domain search.
	- [Zoomeye](https://www.zoomeye.org/) - A Chinese version of Shodan.

To automate we will create a file called sources.txt with the following contents:
```shell-session
baidu
bufferoverun
crtsh
hackertarget
otx
projecdiscovery
rapiddns
sublist3r
threatcrowd
trello
urlscan
vhost
virustotal
zoomeye
```

The following commands will gather information from these sources:
``` shell
export TARGET="facebook.com"
cat sources.txt | while read source; do theHarvester -d "${TARGET}" -b $source -f "${source}_${TARGET}";done
```

Extract and sort all subdomains found:
``` bash
cat *.json | jq -r '.hosts[]' 2>/dev/null | cut -d':' -f 1 | sort -u > "${TARGET}_theHarvester.txt"
```

Merge all passive recon files and display total number:
``` shell
cat facebook.com_*.txt | sort -u > facebook.com_subdomains_passive.txt
cat facebook.com_subdomains_passive.txt | wc -l
```

## Passive Infrastructure Identification

#### [Netcraft](https://sitereport.netcraft.com)
- Get general information about the domain, including date first seen by Netcraft crawlers.
- Information about the netblock owner, hosting company, nameservers, etc.
- Latest IPs used, webserver and target OS

#### [Wayback Machine](http://web.archive.org/)
- Find old versions of sites that may have interesting comments in the source code or files that should not be there.
- We can also use [waybackurls](https://github.com/tomnomnom/waybackurls) to inspect URLs saved by Wayback and look for specific keywords.
	- To install (with go installed)
		- `go install github.com/tomnomnom/waybackurls@latest`
	- To get a list of crawled URLs from a domain with the date it was obtained, add the `-dates` switch.
``` bash
waybackurls -dates https://facebook.com > waybackurls.txt
cat waybackurls.txt
```

___
# Active Information Gathering

## Active Infrastructure Identification

### Web Servers
- If we find out what webserver is running, that can be an indication of what OS is running.
- We can infer Windows version from IIS version:
	- IIS 6 - Windows Server 2003
	- IIS 7-8.5 - Windows Server 2008/2008r2
	- IIS 10 (v1607-v1709) - Windows Server 2016
	- IIS 10 (v1809+) - Windows Server 2019

#### HTTP Headers
- `curl -I "http://${TARGET}"`

- Other headers of note:
	- X-Powered-By: Can tell us what the web app is using (PHP, ASP.NET, JSP, etc)
	- Cookies: Each technology has default cookies
		- .NET - `ASPSESSIONID<RANDOM>=<COOKIE_VALUE>`
		- PHP - `PHPSESSID=<COOKIE_VALUE>`
		- JAVA - `JSESSION=<COOKIE_VALUE>`

- Whatweb
	- Recognizes web technologies.
	- Use `whatweb -h`  to see available options
	- `whatweb -a3 https://www.facebook.com -v`
- [Wappalyzer](https://www.wappalyzer.com/) (Paid with free tier)
	- Browser extension similiar to whatweb 
- [WafW00f](https://github.com/EnableSecurity/wafw00f)
	- Web application firewall fingerprinting tool
	- Sends requests and analyses responses to determine if a security solution is in place.
	- Install: `sudo apt install wafw00f -y`
	- `-a` checks all possible WAFs instead of stopping on the first one
	- `-i` read targets from a file
	- `-p` proxy the requests
- [Aquatone](https://github.com/michenriksen/aquatone)
	- Tool for automatic and visual inspection of websites across many hosts
	- Convenient for quickly getting an overview of HTTP-based attack surfaces
	- Scans ports, screenshots webpages and generates a report of everything scanned
	- Good for large subdomain lists
- To install:
``` bash
sudo apt install golang chromium-driver
go get github.com/michenriksen/aquatone
export PATH="$PATH":"$HOME/go/bin"
```
- Example of using a subdomain list in aquatone
``` bash
cat facebook_aquatone.txt | aquatone -out ./aquatone -screenshot-timeout 1000
```

## Active Subdomain Enumeration

#### Zone Transfers
- If successful, will enumerate all available subdomain information.
- [[DNS#Footprinting DNS|Footprinting DNS]]

- Browser based:
	- https://hackertarget.com/zone-transfer/

- Manual Technique
	- Identify Nameservers
		- `nslookup -type=NS <domain>`
	- Attempt zone transfer
		- `nslookup -type=any -query=AXFR <domain> <name server>`

#### Gobuster
- The pattern options are useful for discovery new subdomains based on a naming pattern.
- To do pattern based searching:
	- Create a` patterns.txt` file will discovered patterns
		- ex: `lert-api-shv-{GOBUSTER}-sin6` or `atlas-pp-shv-{GOBUSTER}-sin6`
	- Launch gobuster using the DNS module with the following options:
		- `dns` - use DNS module
		- `-q` - quiet
		- `-r` - use custom name server
		- `-d` - target domain name
		- `-p` - path to pattern file
		- `-w` - path to wordlist
		- `-o` - output file
- For example:
``` bash
export TARGET="facebook.com"
export NS="d.ns.facebook.com"
export WORDLIST="numbers.txt"

gobuster dns -q -r "${NS}" -d "${TARGET}" -w "${WORDLIST}" -p ./patterns.txt -o "gobuster_${TARGET}.txt"
```

## Virtual Hosts
- A vHost is a feature that allows several website to be hosted on a single server.

#### IP-based Virtual Hosting
- A host can have multiple network interfaces. Multiple IPs or interface aliases can be configured on each NIC of a host.
- Different servers can be addressed under different IPs on the same host.
- From a client perspective, the servers are independent of one another.

#### Name-based Virtual Hosting
- Different named domains under the same IP
- Handling is taken care of in software.
- Because we know the IP we can fuzz hosts using a wordlist. The following script uses content-length differences to ID valid domains.
``` shell
cat vhosts.txt | while read vhost;do echo "\n********\nFUZZING: ${vhost}\n********";curl -s -I http://192.168.10.10 -H "HOST: ${vhost}.randomtarget.com" | grep "Content-Length: ";done
```

To cURL to a different domain at the same IP:
`curl -s http://<IP> -H "Host: sub.domain.com"`

#### Automating Virtual Hosts Discovery
- [ffuf](https://github.com/ffuf/ffuf)
- Options:
```shell-session
MATCHER OPTIONS:
  -mc                 Match HTTP status codes, or "all" for everything. (default: 200,204,301,302,307,401,403,405)
  -ml                 Match amount of lines in response
  -mr                 Match regexp
  -ms                 Match HTTP response size
  -mw                 Match amount of words in response

FILTER OPTIONS:
  -fc                 Filter HTTP status codes from response. Comma separated list of codes and ranges
  -fl                 Filter by amount of lines in response. Comma separated list of line counts and ranges
  -fr                 Filter regexp
  -fs                 Filter HTTP response size. Comma separated list of sizes and ranges
  -fw                 Filter by amount of words in response. Comma separated list of word counts and ranges
```

`ffuf -w ./vhosts -u http://192.168.10.10 -H "HOST: FUZZ.randomtarget.com" -fs <size>`

___ 
# Crawling

## [ZAP](https://www.zaproxy.org/)
- Web proxy with spidering functionality
	- In the top-right, open the in app browser
	- Enter the website in the address bar and add it to the scope using the first entry in the left menu
	- Return to the ZAP window, right-click the target website, click on the Attack menu, and then select Spider.

## FFuF for Crawling
	- `ffuf -recursion -recursion-depth 1 -u http://192.168.10.10/FUZZ -w /path/to/wordlist.txt`

## FFuF for Finding Secrets
- First, pick an extension list from Seclists `raft-[ small | medium | large ]-extensions.txt`
- Make a list of folders found earlier as folders.txt
- Then use [CeWL](https://github.com/digininja/CeWL) to extract some keywords from the website. The following extracts words with a minimum length of 5 characters, converts them to lowercase and saves them to a wordlist.txt
	- `cewl -m5 --lowercase -w wordlist.txt http://192.168.10.10`
- We use the following parameters in ffuf:
	- `-w` - Separate the wordlists by comma and add an alias to them to inject them as fuzzing points later
	- `-u` - Our target with the fuzzing points.

`ffuf -w ./folders.txt:FOLDERS,./wordlist.txt:WORDLIST,./extensions.txt:EXTENSIONS -u http://192.168.10.10/FOLDERS/WORDLISTEXTENSIONS`